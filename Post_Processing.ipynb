{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-16T04:05:52.312654Z",
     "start_time": "2023-06-16T04:05:52.308899Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast, gc, os, warnings, glob\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForTokenClassification, AutoTokenizer\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import dataset_class.dataclass as dataset_class\n",
    "import model.loss as model_loss\n",
    "import model.metric as model_metric\n",
    "import model.model as model_arch\n",
    "from dataset_class import data_preprocessing\n",
    "from dataset_class.data_preprocessing import *\n",
    "from utils.helper import *\n",
    "from trainer.trainer_utils import *\n",
    "from model.metric import *\n",
    "from utils.helper import class2dict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ['LRU_CACHE_CAPACITY'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Configuration Class for LLM, Classifier such as XGBoost, LightGBM, CatBoost \"\"\"\n",
    "\n",
    "class CFG:\n",
    "    wandb = True\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 4\n",
    "    weight_path = './saved/model'\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    n_folds = 5\n",
    "    max_len = 2048\n",
    "    val_batch_size = 64\n",
    "    xgb_params = {\n",
    "         'learning_rate': 0.05,\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 5,\n",
    "        'gamma': 0,\n",
    "        'subsample': 0.7,\n",
    "        'reg_alpha': 0.0005,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'scale_pos_weight': 1,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "    cat_params = {\n",
    "        'iterations': 2000,\n",
    "        'learning_rate': 0.07,\n",
    "        'depth': 12,\n",
    "        'l2_leaf_reg':8 ,\n",
    "        'random_strength':0.5,\n",
    "        'loss_function': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'task_type': 'GPU',\n",
    "        'border_count': 128,\n",
    "        'verbose': 1000,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'use_best_model': True,\n",
    "\n",
    "    }\n",
    "    lgb_params = {\n",
    "    'n_estimators': 1500, # use a large number of trees with early stopping\n",
    "    'max_depth': 12, # restrict the depths of the individual trees\n",
    "    'min_child_samples': 20, # atleast 20 observations in leaf\n",
    "    'early_stopping_round': 50, # this can be specified in config as well\n",
    "    'subsample_freq': 1, # this can be specified in config as well\n",
    "    'n_jobs': 1,\n",
    "    'importance_type': 'gain',\n",
    "    'device': 'gpu'\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T02:12:57.167422Z",
     "start_time": "2023-06-16T02:12:55.524300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\"\"\" torch.cuda, cudnn, reproducibility setting \"\"\"\n",
    "\n",
    "check_library(True)\n",
    "all_type_seed(CFG, True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(CFG.seed)\n",
    "\n",
    "\n",
    "\"\"\" Trainer Class for Make Sequence Dataset for Multiple Label Classification Task Pipeline \"\"\"\n",
    "\n",
    "class SequenceDataTrainer:\n",
    "    \"\"\"\n",
    "    Only Forward Pass with Validation Dataset for Making Sequence Dataset by whole Competition Data\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, generator: torch.Generator) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.model_name = get_name(self.cfg)\n",
    "        self.generator = generator\n",
    "        self.df = load_data('./dataset_class/data_folder/final_converted_train_df.csv')\n",
    "\n",
    "    def make_batch(self, fold: int) -> tuple[torch.utils.data.DataLoader, pd.DataFrame]:\n",
    "        \"\"\" Make Batch Dataset for main train loop \"\"\"\n",
    "        valid = self.df[self.df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Custom Datasets\n",
    "        valid_dataset = getattr(dataset_class, self.cfg.dataset)(self.cfg, valid, is_train=False)\n",
    "        loader_valid = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=self.cfg.val_batch_size,\n",
    "            shuffle=False,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=self.generator,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        return loader_valid, valid\n",
    "\n",
    "    def model_setting(self, path: str):\n",
    "        \"\"\" load fine-tuned model's weight, iterate by fold \"\"\"\n",
    "        model = getattr(model_arch, self.cfg.model_arch)(self.cfg)\n",
    "        model.load_state_dict(\n",
    "            torch.load(path)\n",
    "        )\n",
    "        model.to(self.cfg.device)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def valid_fn(self, loader_valid, model) -> tuple[list, list]:\n",
    "        \"\"\" Validation Functions \"\"\"\n",
    "        ids_to_labels = data_preprocessing.ids2labels()\n",
    "        val_ids_list, val_pred_list, val_label_list = [], [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, (ids, inputs) in enumerate(tqdm(loader_valid)):  # Maybe need to append\n",
    "                inputs = collate(inputs)\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.to(self.cfg.device)  # prompt to GPU\n",
    "\n",
    "                val_ids_list += ids  # make list for calculating cross validation score\n",
    "                val_pred = model(inputs)  # inference for cross validation\n",
    "\n",
    "                flat_val_pred = torch.argmax(val_pred, dim=-1).detach().cpu().numpy()\n",
    "                predictions = []\n",
    "                for k, text_pred in enumerate(flat_val_pred):\n",
    "                    token_pred = [ids_to_labels[i] for i in text_pred]\n",
    "                    prediction = []\n",
    "                    word_ids = inputs['word_ids'][k].detach().cpu().numpy()\n",
    "                    previous_word_idx = -1\n",
    "                    for idx, word_idx in enumerate(word_ids):\n",
    "                        if word_idx == -1:\n",
    "                            pass\n",
    "                        elif word_idx != previous_word_idx:\n",
    "                            prediction.append(token_pred[idx])\n",
    "                            previous_word_idx = word_idx\n",
    "                    predictions.append(prediction)\n",
    "                val_pred_list.extend(predictions)\n",
    "        gc.collect()\n",
    "        return val_ids_list, val_pred_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T02:23:19.375034Z",
     "start_time": "2023-06-16T02:23:19.371323Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Let's Make Sequence Dataset by forwarding each fold's dataset to fold's model weight \"\"\"\n",
    "\n",
    "tmp_valid = pd.read_csv('./dataset_class/data_folder/train.csv')\n",
    "fold_list = glob.glob(f'{CFG.weight_path}/*.pth')\n",
    "all_id_list, all_pred_list = [], []\n",
    "\n",
    "for fold, model_path in tqdm(enumerate(fold_list)):\n",
    "        print(f'============== {fold}th Fold forward ==============')\n",
    "        forward_input = SequenceDataTrainer(CFG, g)\n",
    "        loader_valid, valid = forward_input.make_batch(fold)\n",
    "        fold_model = forward_input.model_setting(model_path)\n",
    "\n",
    "        # forward pass\n",
    "        val_ids_list, val_pred_list = forward_input.valid_fn(loader_valid, fold_model)\n",
    "        all_id_list.extend(val_ids_list), all_pred_list.extend(val_pred_list)\n",
    "\n",
    "print(all_id_list)\n",
    "print(all_pred_list)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\"\"\" loop function for making sequence dataset \"\"\"\n",
    "\n",
    "def val_loop(cfg: any) -> None:\n",
    "    \"\"\" Base Trainer Loop Function \"\"\"\n",
    "    tmp_valid = pd.read_csv('./dataset_class/data_folder/train.csv')\n",
    "    fold_list = [i for i in range(cfg.n_folds)]\n",
    "    for fold in tqdm(fold_list[4:5]):\n",
    "        print(f'============== {fold}th Fold Train & Validation ==============')\n",
    "        wandb.init(\n",
    "            project=cfg.name,\n",
    "            name=f'FBP2_fold{fold}/' + cfg.model,\n",
    "            config=class2dict(cfg),\n",
    "            group=f'FBP2/{cfg.model}',\n",
    "            job_type='train',\n",
    "            entity=\"qcqced\"\n",
    "        )\n",
    "        early_stopping = EarlyStopping(mode=cfg.stop_mode, patience=3)\n",
    "        early_stopping.detecting_anomaly()\n",
    "\n",
    "        val_score_max = get_save_thresholds(cfg)\n",
    "        train_input = getattr(trainer, cfg.name)(cfg, g)  # init object\n",
    "        loader_train, loader_valid, train, valid = train_input.make_batch(fold)\n",
    "        model, criterion, val_metrics, optimizer, lr_scheduler = train_input.model_setting(len(train))\n",
    "\n",
    "        for epoch in range(cfg.epochs):\n",
    "            print(f'[{epoch + 1}/{cfg.epochs}] Train & Validation')\n",
    "            train_loss, train_accuracy, train_recall, train_precision = train_input.train_fn(\n",
    "                loader_train, model, criterion, optimizer, lr_scheduler, val_metrics\n",
    "            )\n",
    "            val_ids_list, val_pred_list = train_input.valid_fn(\n",
    "                loader_valid, model\n",
    "            )\n",
    "            # 1) make prediction dataframe\n",
    "            final_pred = []\n",
    "            for i in range(len(valid)):\n",
    "                idx = valid.id.values[i]\n",
    "                pred = val_pred_list[i]\n",
    "                tmp_pred = []\n",
    "                j = 0\n",
    "                while j < len(pred):\n",
    "                    cls = pred[j]\n",
    "                    if cls == 'O':\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        cls = cls.replace('B', 'I')  # spans start with B\n",
    "                    end = j + 1\n",
    "                    while end < len(pred) and pred[end] == cls:\n",
    "                        end += 1\n",
    "\n",
    "                    if cls != 'O' and cls != '' and end - j > 7:\n",
    "                        final_pred.append(\n",
    "                            (idx, cls.replace('I-', ''),\n",
    "                             ' '.join(map(str, list(range(j, end)))))\n",
    "                        )\n",
    "                    j = end\n",
    "            pred_df = pd.DataFrame(final_pred)\n",
    "            pred_df.columns = ['id', 'class', 'predictionstring']\n",
    "\n",
    "            # 2) calculate cross validation score\n",
    "            batch_valid = tmp_valid.loc[tmp_valid['id'].isin(val_ids_list)].copy()\n",
    "            f1_list = []\n",
    "            unique_class = pred_df['class'].unique()\n",
    "            for i, c in enumerate(unique_class):\n",
    "                print(f'iteration: {i}, class: {c}')\n",
    "                subset_pred_df = pred_df.loc[pred_df['class'] == c].copy()\n",
    "                gt_df = batch_valid.loc[batch_valid['discourse_type'] == c].copy()\n",
    "                f1_score = calculate_f1(subset_pred_df, gt_df)\n",
    "                print(c, f1_score)  # print f1 score for each class\n",
    "                f1_list.append(f1_score)\n",
    "            final_f1_score = np.mean(f1_list)  # average == 'micro'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T04:11:24.783932Z",
     "start_time": "2023-06-16T04:11:24.779862Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
