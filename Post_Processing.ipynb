{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-06-19T05:32:05.170619Z",
     "start_time": "2023-06-19T05:32:00.469521Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/qcqced/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import ast, gc, os, warnings, glob\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForTokenClassification, AutoTokenizer\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import dataset_class.dataclass as dataset_class\n",
    "import model.loss as model_loss\n",
    "import model.metric as model_metric\n",
    "import model.model as model_arch\n",
    "from model.model_utils import *\n",
    "from dataset_class import data_preprocessing\n",
    "from dataset_class.data_preprocessing import *\n",
    "from utils.helper import *\n",
    "from trainer.trainer_utils import *\n",
    "from model.metric import *\n",
    "from utils.helper import class2dict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ['LRU_CACHE_CAPACITY'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-19T05:32:06.722377Z",
     "start_time": "2023-06-19T05:32:05.173507Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Configuration Class for LLM, Classifier such as XGBoost, LightGBM, CatBoost \"\"\"\n",
    "\n",
    "class CFG:\n",
    "    wandb = True\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    weight_path = './saved/model'\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "    reinit = True\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    n_folds = 5\n",
    "    max_len = 2048\n",
    "    val_batch_size = 16\n",
    "    xgb_params = {\n",
    "         'learning_rate': 0.05,\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 5,\n",
    "        'gamma': 0,\n",
    "        'subsample': 0.7,\n",
    "        'reg_alpha': 0.0005,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'scale_pos_weight': 1,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "    cat_params = {\n",
    "        'iterations': 2000,\n",
    "        'learning_rate': 0.07,\n",
    "        'depth': 12,\n",
    "        'l2_leaf_reg':8 ,\n",
    "        'random_strength':0.5,\n",
    "        'loss_function': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'task_type': 'GPU',\n",
    "        'border_count': 128,\n",
    "        'verbose': 1000,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'use_best_model': True,\n",
    "\n",
    "    }\n",
    "    lgb_params = {\n",
    "    'n_estimators': 1500, # use a large number of trees with early stopping\n",
    "    'max_depth': 12, # restrict the depths of the individual trees\n",
    "    'min_child_samples': 20, # atleast 20 observations in leaf\n",
    "    'early_stopping_round': 50, # this can be specified in config as well\n",
    "    'subsample_freq': 1, # this can be specified in config as well\n",
    "    'n_jobs': 1,\n",
    "    'importance_type': 'gain',\n",
    "    'device': 'gpu'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-19T05:32:06.731162Z",
     "start_time": "2023-06-19T05:32:06.728174Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom Dataset Class \"\"\"\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset Class for NER Task\n",
    "    Args:\n",
    "        cfg: configuration.CFG\n",
    "        df: dataframe from .txt file\n",
    "        is_train: if this param set False, return word_ids from self.df.entities\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: configuration.CFG, df: pd.DataFrame, is_train: bool = True) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = ner_tokenizing\n",
    "        self.labels2ids = labels2ids()  # Function for Encoding Labels to ids\n",
    "        self.ids2labels = ids2labels()  # Function for Decoding ids to Labels\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item: int) -> tuple[list, [dict[Tensor, Tensor, Tensor], Tensor]]:\n",
    "        \"\"\"\n",
    "        1) Tokenizing input text:\n",
    "            - if you param 'return_offsets_mapping' == True, tokenizer doen't erase \\n or \\n\\n\n",
    "              but, I don't know this param also applying for DeBERTa Pretrained Tokenizer\n",
    "        2) Create targets and mapping of tokens to split() words by tokenizer\n",
    "            - Mapping Labels to split tokens\n",
    "            - Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "            - Tokenizer will split word into subsequent of character such as copied => copy, ##ed\n",
    "            - So, we need to find having same parent token and then label BIO NER Tags\n",
    "        3) Return dict:\n",
    "            - Train: dict.keys = [inputs_id, attention_mask, token_type_ids, labels]\n",
    "            - Validation/Test: dict.keys = [inputs_id, attention_mask, token_type_ids, word_ids]\n",
    "        \"\"\"\n",
    "        ids = self.df.id[item]\n",
    "        text = self.df.text[item]\n",
    "        if self.is_train:\n",
    "            word_labels = ast.literal_eval(self.df.entities[item])\n",
    "\n",
    "        # 1) Tokenizing input text\n",
    "        encoding = self.tokenizer(\n",
    "            self.cfg,\n",
    "            text,\n",
    "        )\n",
    "        word_ids = encoding.word_ids()\n",
    "        split_word_ids = np.full(len(word_ids), -1)\n",
    "        offset_to_wordidx = split_mapping(text)  # [1, sequence_length]\n",
    "        offsets = encoding['offset_mapping']  # [(src, end), (src, end), ...]\n",
    "\n",
    "        # 2) Find having same parent token and then label BIO NER Tags\n",
    "        label_ids = []\n",
    "        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "            if word_idx is None:\n",
    "                \"\"\" for padding token \"\"\"\n",
    "                if self.is_train:\n",
    "                    label_ids.append(-100)\n",
    "            else:\n",
    "                if offsets[token_idx] != (0, 0):\n",
    "                    # Choose the split word that shares the most characters with the token if any\n",
    "                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(\n",
    "                        np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "                    if split_index != -1:\n",
    "                        if self.is_train:\n",
    "                            label_ids.append(self.labels2ids[word_labels[split_index]])\n",
    "                        split_word_ids[token_idx] = split_index\n",
    "                    else:\n",
    "                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                        if label_ids and label_ids[-1] != -100 and self.ids2labels[label_ids[-1]][0] == 'I':\n",
    "                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                            if self.is_train:\n",
    "                                label_ids.append(label_ids[-1])\n",
    "                        else:\n",
    "                            if self.is_train:\n",
    "                                label_ids.append(-100)\n",
    "                else:\n",
    "                    if self.is_train:\n",
    "                        label_ids.append(-100)\n",
    "        if not self.is_train:\n",
    "            encoding['word_ids'] = torch.as_tensor(split_word_ids)\n",
    "        else:\n",
    "            encoding['labels'] = list(reversed(label_ids))\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = torch.as_tensor(v)\n",
    "        return ids, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-19T05:32:06.741510Z",
     "start_time": "2023-06-19T05:32:06.732953Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom Model Class \"\"\"\n",
    "\n",
    "class DeBERTaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model class For NER Task Pipeline, in this class no pooling layer with backbone named \"DeBERTa\"\n",
    "    This pipeline apply B.I.O Style, so the number of classes is 15 which is 7 unique classes original\n",
    "    Each of 7 unique classes has sub 2 classes (B, I) => 14 classes\n",
    "    And 1 class for O => 1 class\n",
    "    14 + 1 = 15 classes\n",
    "    Args:\n",
    "        cfg: configuration.CFG\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: configuration.CFG) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.auto_cfg = AutoConfig.from_pretrained(\n",
    "            cfg.model,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            cfg.model,\n",
    "            config=self.auto_cfg\n",
    "        )\n",
    "        self.fc = nn.Linear(self.auto_cfg.hidden_size, 15)  # BIO Style NER Task\n",
    "\n",
    "    def feature(self, inputs_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(\n",
    "            input_ids=inputs_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs) -> Tensor:\n",
    "        \"\"\"\n",
    "        No Pooling Layer for word-level task\n",
    "        Args:\n",
    "            inputs: Dict type from AutoTokenizer\n",
    "            => {input_ids, attention_mask, token_type_ids, offset_mapping, labels}\n",
    "        \"\"\"\n",
    "        outputs = self.feature(\n",
    "            inputs_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            token_type_ids=inputs[\"token_type_ids\"],\n",
    "        )\n",
    "        logit = self.fc(outputs.last_hidden_state)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-19T05:32:06.747374Z",
     "start_time": "2023-06-19T05:32:06.742335Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" torch.cuda, cudnn, reproducibility setting \"\"\"\n",
    "\n",
    "check_library(True)\n",
    "all_type_seed(CFG, True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(CFG.seed)\n",
    "\n",
    "\n",
    "\"\"\" Trainer Class for Make Sequence Dataset for Multiple Label Classification Task Pipeline \"\"\"\n",
    "\n",
    "class SequenceDataTrainer:\n",
    "    \"\"\"\n",
    "    Only Forward Pass with Validation Dataset for Making Sequence Dataset by whole Competition Data\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, generator: torch.Generator) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.model_name = get_name(self.cfg)\n",
    "        self.generator = generator\n",
    "        self.df = load_data('./dataset_class/data_folder/final_converted_train_df.csv')\n",
    "\n",
    "    def make_batch(self, fold: int) -> tuple[torch.utils.data.DataLoader, pd.DataFrame]:\n",
    "        \"\"\" Make Batch Dataset for main train loop \"\"\"\n",
    "        valid = self.df[self.df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Custom Datasets\n",
    "        valid_dataset = NERDataset(self.cfg, valid, is_train=False)\n",
    "        loader_valid = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=self.cfg.val_batch_size,\n",
    "            shuffle=False,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=self.generator,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        return loader_valid, valid\n",
    "\n",
    "    def model_setting(self, path: str):\n",
    "        \"\"\" load fine-tuned model's weight, iterate by fold \"\"\"\n",
    "        model = DeBERTaModel(self.cfg)\n",
    "        model.load_state_dict(\n",
    "            torch.load(path, map_location=torch.device('mps'))\n",
    "        )\n",
    "        model.to(self.cfg.device)\n",
    "        return model\n",
    "\n",
    "    def inference_fn(self, loader_valid: torch.utils.data.DataLoader, model: nn.Module) -> tuple[list, list]:\n",
    "        \"\"\"\n",
    "        Validation Functions\n",
    "        Not convert probability to string label text with torch.argmax\n",
    "        function should return those shape of Tensor: [batch_size, sequence_length, num_labels] == outputs.last_hidden_state\n",
    "        Variable:\n",
    "            val_ids_list: list of ids for calculating sequence dataset\n",
    "            val_prob_list: list of probability for make sequence dataset\n",
    "            val_label_list: list of labels for calculating CV Score\n",
    "        \"\"\"\n",
    "        word_ids = np.full((len(loader_valid.dataset), CFG.max_len), -100)\n",
    "        val_ids_list, val_prob_list, val_label_list = [], [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, (ids, inputs) in enumerate(tqdm(loader_valid)):  # Maybe need to append\n",
    "                inputs = collate(inputs)\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.to(self.cfg.device)  # prompt to GPU\n",
    "\n",
    "                val_pred = model(inputs)  # [batch_size, sequence_length, num_labels]\n",
    "                val_prob = F.softmax(val_pred, dim=2).cpu().detach().numpy()  # dim 2 == num_labels dim\n",
    "\n",
    "                val_prob_list.extend(val_prob), val_ids_list.extend(ids)  # make list for sequence dataset\n",
    "\n",
    "            predictions = []\n",
    "            for idx in range(val_prob_list.shape[0]):\n",
    "                \"\"\" loop for each unique ids \"\"\"\n",
    "                prediction, prob_buffer, previous_word_idx = [], [], -1\n",
    "                sequence_logit = val_prob_list[idx]\n",
    "                sub_ids = word_ids[idx][word_ids[idx] != -100]\n",
    "                for i, word_idx in enumerate(sub_ids):\n",
    "                    if word_idx == -1:\n",
    "                        pass\n",
    "                    elif word_idx != previous_word_idx:\n",
    "                        if prob_buffer:\n",
    "                            prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "                            prob_buffer = []\n",
    "                        prob_buffer.append(sequence_logit[i])\n",
    "                        previous_word_idx = word_idx\n",
    "                    else:\n",
    "                        prob_buffer.append(sequence_logit[i])\n",
    "                prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "                predictions.append(prediction)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-19T05:34:00.331185Z",
     "start_time": "2023-06-19T05:32:06.749187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c16e4c4d729454d91290e290b33a121"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 0th Fold forward ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/195 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acf10d73479e46d09fc720f37b1f0bbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m fold_model \u001B[38;5;241m=\u001B[39m forward_input\u001B[38;5;241m.\u001B[39mmodel_setting(model_path)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# forward pass\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mforward_input\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minference_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloader_valid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfold_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(predictions)\n\u001B[1;32m     19\u001B[0m all_pred_list\u001B[38;5;241m.\u001B[39mextend(predictions)\n",
      "Cell \u001B[0;32mIn[5], line 67\u001B[0m, in \u001B[0;36mSequenceDataTrainer.inference_fn\u001B[0;34m(self, loader_valid, model)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     65\u001B[0m     inputs[k] \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# prompt to GPU\u001B[39;00m\n\u001B[0;32m---> 67\u001B[0m val_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# [batch_size, sequence_length, num_labels]\u001B[39;00m\n\u001B[1;32m     68\u001B[0m val_prob \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(val_pred, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()  \u001B[38;5;66;03m# dim 2 == num_labels dim\u001B[39;00m\n\u001B[1;32m     70\u001B[0m val_prob_list\u001B[38;5;241m.\u001B[39mextend(val_prob), val_ids_list\u001B[38;5;241m.\u001B[39mextend(ids)  \u001B[38;5;66;03m# make list for sequence dataset\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[4], line 41\u001B[0m, in \u001B[0;36mDeBERTaModel.forward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m     35\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;03m    No Pooling Layer for word-level task\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m        inputs: Dict type from AutoTokenizer\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m        => {input_ids, attention_mask, token_type_ids, offset_mapping, labels}\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtoken_type_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m     logit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state)\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logit\n",
      "Cell \u001B[0;32mIn[4], line 27\u001B[0m, in \u001B[0;36mDeBERTaModel.feature\u001B[0;34m(self, inputs_ids, attention_mask, token_type_ids)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeature\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs_ids, attention_mask, token_type_ids):\n\u001B[0;32m---> 27\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1082\u001B[0m, in \u001B[0;36mDebertaV2Model.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1072\u001B[0m     token_type_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(input_shape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m   1074\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m   1075\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1076\u001B[0m     token_type_ids\u001B[38;5;241m=\u001B[39mtoken_type_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1079\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m   1080\u001B[0m )\n\u001B[0;32m-> 1082\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1083\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1084\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1085\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1086\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1087\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1088\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1089\u001B[0m encoded_layers \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   1091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mz_steps \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:520\u001B[0m, in \u001B[0;36mDebertaV2Encoder.forward\u001B[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001B[0m\n\u001B[1;32m    511\u001B[0m     output_states \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    512\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    513\u001B[0m         next_kv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    517\u001B[0m         rel_embeddings,\n\u001B[1;32m    518\u001B[0m     )\n\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 520\u001B[0m     output_states \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnext_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrelative_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrelative_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrel_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrel_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n\u001B[1;32m    530\u001B[0m     output_states, att_m \u001B[38;5;241m=\u001B[39m output_states\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:362\u001B[0m, in \u001B[0;36mDebertaV2Layer.forward\u001B[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001B[0m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    354\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    355\u001B[0m     hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    360\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    361\u001B[0m ):\n\u001B[0;32m--> 362\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrelative_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrelative_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrel_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrel_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n\u001B[1;32m    371\u001B[0m         attention_output, att_matrix \u001B[38;5;241m=\u001B[39m attention_output\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:305\u001B[0m, in \u001B[0;36mDebertaV2Attention.forward\u001B[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m query_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    304\u001B[0m     query_states \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m--> 305\u001B[0m attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mself_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n\u001B[1;32m    308\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (attention_output, att_matrix)\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:270\u001B[0m, in \u001B[0;36mDebertaV2SelfOutput.forward\u001B[0;34m(self, hidden_states, input_tensor)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states, input_tensor):\n\u001B[0;32m--> 270\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    271\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(hidden_states)\n\u001B[1;32m    272\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_states \u001B[38;5;241m+\u001B[39m input_tensor)\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's Make Sequence Dataset by forwarding each fold's dataset to fold's model weight\n",
    "loop function for Sequence Dataset Generate\n",
    "\"\"\"\n",
    "\n",
    "tmp_valid = pd.read_csv('./dataset_class/data_folder/train.csv')\n",
    "fold_list = glob.glob(f'{CFG.weight_path}/*.pth')\n",
    "all_id_list, all_pred_list = [], []\n",
    "\n",
    "for fold, model_path in tqdm(enumerate(fold_list)):\n",
    "        print(f'============== {fold}th Fold forward ==============')\n",
    "        forward_input = SequenceDataTrainer(CFG, g)\n",
    "        loader_valid, valid = forward_input.make_batch(fold)\n",
    "        fold_model = forward_input.model_setting(model_path)\n",
    "\n",
    "        # forward pass\n",
    "        predictions = forward_input.inference_fn(loader_valid, fold_model)\n",
    "        print(predictions)\n",
    "        all_pred_list.extend(predictions)\n",
    "\n",
    "uniqueValidGroups = range(len(all_pred_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
