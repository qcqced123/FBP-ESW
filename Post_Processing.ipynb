{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-06-19T05:36:15.597712771Z",
     "start_time": "2023-06-19T05:36:10.457257719Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 14:36:12.984944: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-19 14:36:13.667940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/qcqced/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import ast, gc, os, warnings, glob\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForTokenClassification, AutoTokenizer\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import dataset_class.dataclass as dataset_class\n",
    "import model.loss as model_loss\n",
    "import model.metric as model_metric\n",
    "import model.model as model_arch\n",
    "from model.model_utils import *\n",
    "from dataset_class import data_preprocessing\n",
    "from dataset_class.data_preprocessing import *\n",
    "from utils.helper import *\n",
    "from trainer.trainer_utils import *\n",
    "from model.metric import *\n",
    "from utils.helper import class2dict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ['LRU_CACHE_CAPACITY'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-19T05:36:19.839394090Z",
     "start_time": "2023-06-19T05:36:18.947454721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Configuration Class for LLM, Classifier such as XGBoost, LightGBM, CatBoost \"\"\"\n",
    "\n",
    "class CFG:\n",
    "    wandb = True\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    weight_path = './saved/model'\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "    reinit = True\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    n_folds = 5\n",
    "    max_len = 2048\n",
    "    val_batch_size = 16\n",
    "    xgb_params = {\n",
    "         'learning_rate': 0.05,\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 5,\n",
    "        'gamma': 0,\n",
    "        'subsample': 0.7,\n",
    "        'reg_alpha': 0.0005,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'scale_pos_weight': 1,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "    cat_params = {\n",
    "        'iterations': 2000,\n",
    "        'learning_rate': 0.07,\n",
    "        'depth': 12,\n",
    "        'l2_leaf_reg':8 ,\n",
    "        'random_strength':0.5,\n",
    "        'loss_function': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'task_type': 'GPU',\n",
    "        'border_count': 128,\n",
    "        'verbose': 1000,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'use_best_model': True,\n",
    "\n",
    "    }\n",
    "    lgb_params = {\n",
    "    'n_estimators': 1500, # use a large number of trees with early stopping\n",
    "    'max_depth': 12, # restrict the depths of the individual trees\n",
    "    'min_child_samples': 20, # atleast 20 observations in leaf\n",
    "    'early_stopping_round': 50, # this can be specified in config as well\n",
    "    'subsample_freq': 1, # this can be specified in config as well\n",
    "    'n_jobs': 1,\n",
    "    'importance_type': 'gain',\n",
    "    'device': 'gpu'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-19T05:36:21.135605015Z",
     "start_time": "2023-06-19T05:36:21.118024132Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom Dataset Class \"\"\"\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset Class for NER Task\n",
    "    Args:\n",
    "        cfg: configuration.CFG\n",
    "        df: dataframe from .txt file\n",
    "        is_train: if this param set False, return word_ids from self.df.entities\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: configuration.CFG, df: pd.DataFrame, is_train: bool = True) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.tokenizer = ner_tokenizing\n",
    "        self.labels2ids = labels2ids()  # Function for Encoding Labels to ids\n",
    "        self.ids2labels = ids2labels()  # Function for Decoding ids to Labels\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item: int) -> tuple[list, [dict[Tensor, Tensor, Tensor], Tensor]]:\n",
    "        \"\"\"\n",
    "        1) Tokenizing input text:\n",
    "            - if you param 'return_offsets_mapping' == True, tokenizer doen't erase \\n or \\n\\n\n",
    "              but, I don't know this param also applying for DeBERTa Pretrained Tokenizer\n",
    "        2) Create targets and mapping of tokens to split() words by tokenizer\n",
    "            - Mapping Labels to split tokens\n",
    "            - Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "            - Tokenizer will split word into subsequent of character such as copied => copy, ##ed\n",
    "            - So, we need to find having same parent token and then label BIO NER Tags\n",
    "        3) Return dict:\n",
    "            - Train: dict.keys = [inputs_id, attention_mask, token_type_ids, labels]\n",
    "            - Validation/Test: dict.keys = [inputs_id, attention_mask, token_type_ids, word_ids]\n",
    "        \"\"\"\n",
    "        ids = self.df.id[item]\n",
    "        text = self.df.text[item]\n",
    "        if self.is_train:\n",
    "            word_labels = ast.literal_eval(self.df.entities[item])\n",
    "\n",
    "        # 1) Tokenizing input text\n",
    "        encoding = self.tokenizer(\n",
    "            self.cfg,\n",
    "            text,\n",
    "        )\n",
    "        word_ids = encoding.word_ids()\n",
    "        split_word_ids = np.full(len(word_ids), -1)\n",
    "        offset_to_wordidx = split_mapping(text)  # [1, sequence_length]\n",
    "        offsets = encoding['offset_mapping']  # [(src, end), (src, end), ...]\n",
    "\n",
    "        # 2) Find having same parent token and then label BIO NER Tags\n",
    "        label_ids = []\n",
    "        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "            if word_idx is None:\n",
    "                \"\"\" for padding token \"\"\"\n",
    "                if self.is_train:\n",
    "                    label_ids.append(-100)\n",
    "            else:\n",
    "                if offsets[token_idx] != (0, 0):\n",
    "                    # Choose the split word that shares the most characters with the token if any\n",
    "                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(\n",
    "                        np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "                    if split_index != -1:\n",
    "                        if self.is_train:\n",
    "                            label_ids.append(self.labels2ids[word_labels[split_index]])\n",
    "                        split_word_ids[token_idx] = split_index\n",
    "                    else:\n",
    "                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                        if label_ids and label_ids[-1] != -100 and self.ids2labels[label_ids[-1]][0] == 'I':\n",
    "                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                            if self.is_train:\n",
    "                                label_ids.append(label_ids[-1])\n",
    "                        else:\n",
    "                            if self.is_train:\n",
    "                                label_ids.append(-100)\n",
    "                else:\n",
    "                    if self.is_train:\n",
    "                        label_ids.append(-100)\n",
    "        if not self.is_train:\n",
    "            encoding['word_ids'] = torch.as_tensor(split_word_ids)\n",
    "        else:\n",
    "            encoding['labels'] = list(reversed(label_ids))\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = torch.as_tensor(v)\n",
    "        return ids, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-19T05:36:21.907904762Z",
     "start_time": "2023-06-19T05:36:21.904516011Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom Model Class \"\"\"\n",
    "\n",
    "class DeBERTaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model class For NER Task Pipeline, in this class no pooling layer with backbone named \"DeBERTa\"\n",
    "    This pipeline apply B.I.O Style, so the number of classes is 15 which is 7 unique classes original\n",
    "    Each of 7 unique classes has sub 2 classes (B, I) => 14 classes\n",
    "    And 1 class for O => 1 class\n",
    "    14 + 1 = 15 classes\n",
    "    Args:\n",
    "        cfg: configuration.CFG\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: configuration.CFG) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.auto_cfg = AutoConfig.from_pretrained(\n",
    "            cfg.model,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            cfg.model,\n",
    "            config=self.auto_cfg\n",
    "        )\n",
    "        self.fc = nn.Linear(self.auto_cfg.hidden_size, 15)  # BIO Style NER Task\n",
    "\n",
    "    def feature(self, inputs_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(\n",
    "            input_ids=inputs_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs) -> Tensor:\n",
    "        \"\"\"\n",
    "        No Pooling Layer for word-level task\n",
    "        Args:\n",
    "            inputs: Dict type from AutoTokenizer\n",
    "            => {input_ids, attention_mask, token_type_ids, offset_mapping, labels}\n",
    "        \"\"\"\n",
    "        outputs = self.feature(\n",
    "            inputs_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            token_type_ids=inputs[\"token_type_ids\"],\n",
    "        )\n",
    "        logit = self.fc(outputs.last_hidden_state)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-06-19T05:37:54.880550950Z",
     "start_time": "2023-06-19T05:37:54.839914778Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" torch.cuda, cudnn, reproducibility setting \"\"\"\n",
    "\n",
    "check_library(True)\n",
    "all_type_seed(CFG, True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(CFG.seed)\n",
    "\n",
    "\n",
    "\"\"\" Trainer Class for Make Sequence Dataset for Multiple Label Classification Task Pipeline \"\"\"\n",
    "\n",
    "class SequenceDataTrainer:\n",
    "    \"\"\"\n",
    "    Only Forward Pass with Validation Dataset for Making Sequence Dataset by whole Competition Data\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, generator: torch.Generator) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.model_name = get_name(self.cfg)\n",
    "        self.generator = generator\n",
    "        self.df = load_data('./dataset_class/data_folder/final_converted_train_df.csv')\n",
    "\n",
    "    def make_batch(self, fold: int) -> tuple[torch.utils.data.DataLoader, pd.DataFrame]:\n",
    "        \"\"\" Make Batch Dataset for main train loop \"\"\"\n",
    "        valid = self.df[self.df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Custom Datasets\n",
    "        valid_dataset = NERDataset(self.cfg, valid, is_train=False)\n",
    "        loader_valid = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=self.cfg.val_batch_size,\n",
    "            shuffle=False,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=self.generator,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        return loader_valid, valid\n",
    "\n",
    "    def model_setting(self, path: str):\n",
    "        \"\"\" load fine-tuned model's weight, iterate by fold \"\"\"\n",
    "        model = DeBERTaModel(self.cfg)\n",
    "        # model.load_state_dict(\n",
    "        #     torch.load(path, map_location=torch.device('mps'))\n",
    "        # )\n",
    "        model.load_state_dict(\n",
    "            torch.load(path)\n",
    "        )\n",
    "        model.to(self.cfg.device)\n",
    "        return model\n",
    "\n",
    "    def inference_fn(self, loader_valid: torch.utils.data.DataLoader, model: nn.Module) -> tuple[list, list]:\n",
    "        \"\"\"\n",
    "        Validation Functions\n",
    "        Not convert probability to string label text with torch.argmax\n",
    "        function should return those shape of Tensor: [batch_size, sequence_length, num_labels] == outputs.last_hidden_state\n",
    "        Variable:\n",
    "            val_ids_list: list of ids for calculating sequence dataset\n",
    "            val_prob_list: list of probability for make sequence dataset\n",
    "            val_label_list: list of labels for calculating CV Score\n",
    "        \"\"\"\n",
    "        word_ids = np.full((len(loader_valid.dataset), CFG.max_len), -100)\n",
    "        val_ids_list, val_prob_list, val_label_list = [], [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, (ids, inputs) in enumerate(tqdm(loader_valid)):  # Maybe need to append\n",
    "                inputs = collate(inputs)\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.to(self.cfg.device)  # prompt to GPU\n",
    "\n",
    "                val_pred = model(inputs)  # [batch_size, sequence_length, num_labels]\n",
    "                val_prob = F.softmax(val_pred, dim=2).cpu().detach().numpy()  # dim 2 == num_labels dim\n",
    "\n",
    "                val_prob_list.extend(val_prob), val_ids_list.extend(ids)  # make list for sequence dataset\n",
    "\n",
    "            predictions = []\n",
    "            for idx in range(val_prob_list.shape[0]):\n",
    "                \"\"\" loop for each unique ids \"\"\"\n",
    "                prediction, prob_buffer, previous_word_idx = [], [], -1\n",
    "                sequence_logit = val_prob_list[idx]\n",
    "                sub_ids = word_ids[idx][word_ids[idx] != -100]\n",
    "                for i, word_idx in enumerate(sub_ids):\n",
    "                    if word_idx == -1:\n",
    "                        pass\n",
    "                    elif word_idx != previous_word_idx:\n",
    "                        if prob_buffer:\n",
    "                            prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "                            prob_buffer = []\n",
    "                        prob_buffer.append(sequence_logit[i])\n",
    "                        previous_word_idx = word_idx\n",
    "                    else:\n",
    "                        prob_buffer.append(sequence_logit[i])\n",
    "                prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "                print(prediction)\n",
    "                predictions.append(prediction)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-19T05:37:55.319887823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52c8e727580846d5b12718f59f11c833"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 0th Fold forward ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/195 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e3ca851aa0248028f48f812e21a6f56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's Make Sequence Dataset by forwarding each fold's dataset to fold's model weight\n",
    "loop function for Sequence Dataset Generate\n",
    "\"\"\"\n",
    "\n",
    "tmp_valid = pd.read_csv('./dataset_class/data_folder/train.csv')\n",
    "fold_list = glob.glob(f'{CFG.weight_path}/*.pth')\n",
    "all_id_list, all_pred_list = [], []\n",
    "\n",
    "for fold, model_path in tqdm(enumerate(fold_list)):\n",
    "        print(f'============== {fold}th Fold forward ==============')\n",
    "        forward_input = SequenceDataTrainer(CFG, g)\n",
    "        loader_valid, valid = forward_input.make_batch(fold)\n",
    "        fold_model = forward_input.model_setting(model_path)\n",
    "\n",
    "        # forward pass\n",
    "        predictions = forward_input.inference_fn(loader_valid, fold_model)\n",
    "        print(predictions)\n",
    "        all_pred_list.extend(predictions)\n",
    "\n",
    "uniqueValidGroups = range(len(all_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
